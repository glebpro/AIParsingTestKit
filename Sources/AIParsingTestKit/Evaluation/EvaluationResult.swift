//
//  EvaluationResult.swift
//  AIParsingTestKit
//
//  Represents the result of evaluating a single test case.
//

import Foundation

/// The result of evaluating a single test case against the LLM.
///
/// Contains:
/// - The original test case
/// - The actual output from the LLM (if successful)
/// - Computed metrics from comparing actual vs expected
/// - Performance data (latency)
/// - Error information (if failed)
public struct EvaluationResult<T: EvaluatableTestCase>: Sendable {
    /// The test case that was evaluated.
    public let testCase: T

    /// The actual output from the LLM, if generation succeeded.
    public let actualOutput: T.Expected.ActualOutput?

    /// Time taken for the LLM to respond, in milliseconds.
    public let latencyMs: Double

    /// Error message if generation failed.
    public let error: String?

    /// Metrics computed by comparing actual output to expected.
    public let metrics: [String: MetricValue]

    /// The status of this evaluation.
    public var status: EvaluationStatus {
        if actualOutput == nil {
            return error != nil ? .error : .skipped
        }
        return .completed
    }

    /// Creates a successful evaluation result.
    ///
    /// - Parameters:
    ///   - testCase: The test case that was evaluated.
    ///   - actualOutput: The output generated by the LLM.
    ///   - latencyMs: Time taken in milliseconds.
    public init(
        testCase: T,
        actualOutput: T.Expected.ActualOutput,
        latencyMs: Double
    ) {
        self.testCase = testCase
        self.actualOutput = actualOutput
        self.latencyMs = latencyMs
        self.error = nil
        self.metrics = testCase.expected.compare(to: actualOutput)
    }

    /// Creates a failed evaluation result.
    ///
    /// - Parameters:
    ///   - testCase: The test case that was evaluated.
    ///   - error: Description of what went wrong.
    ///   - latencyMs: Time taken before failure.
    public init(
        testCase: T,
        error: String,
        latencyMs: Double = 0
    ) {
        self.testCase = testCase
        self.actualOutput = nil
        self.latencyMs = latencyMs
        self.error = error
        self.metrics = [:]
    }

    /// Creates a skipped evaluation result (e.g., model unavailable).
    ///
    /// - Parameter testCase: The test case that was skipped.
    public init(skipped testCase: T) {
        self.testCase = testCase
        self.actualOutput = nil
        self.latencyMs = 0
        self.error = nil
        self.metrics = [:]
    }
}

/// The status of an evaluation.
public enum EvaluationStatus: String, Sendable {
    /// The evaluation completed successfully.
    case completed = "COMPLETED"

    /// The evaluation was skipped (e.g., model unavailable).
    case skipped = "SKIPPED"

    /// The evaluation failed with an error.
    case error = "ERROR"
}
